NAIVE BAYES;-----;LOGISTIC REGRESSION;-----
accuracy;-----;accuracy;-----
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.373
             2          -1.30626        0.373
             3          -1.25174        0.539
             4          -1.20139        0.577
             5          -1.15506        0.603
             6          -1.11254        0.629
             7          -1.07358        0.651
             8          -1.03790        0.661
             9          -1.00521        0.664
            10          -0.97523        0.672
            11          -0.94772        0.679
            12          -0.92242        0.688
            13          -0.89911        0.694
            14          -0.87761        0.696
            15          -0.85773        0.708
            16          -0.83931        0.712
            17          -0.82221        0.712
            18          -0.80631        0.714
            19          -0.79150        0.716
            20          -0.77767        0.716
            21          -0.76473        0.718
            22          -0.75261        0.720
            23          -0.74123        0.723
            24          -0.73054        0.725
            25          -0.72047        0.725
            26          -0.71097        0.727
            27          -0.70200        0.727
            28          -0.69352        0.725
            29          -0.68548        0.725
            30          -0.67786        0.725
            31          -0.67062        0.725
            32          -0.66374        0.723
            33          -0.65719        0.723
            34          -0.65094        0.723
            35          -0.64498        0.723
            36          -0.63929        0.723
            37          -0.63385        0.725
            38          -0.62864        0.727
            39          -0.62364        0.729
            40          -0.61886        0.729
            41          -0.61426        0.731
            42          -0.60985        0.731
            43          -0.60560        0.731
            44          -0.60151        0.727
            45          -0.59758        0.727
            46          -0.59379        0.727
            47          -0.59013        0.727
            48          -0.58660        0.727
            49          -0.58319        0.725
            50          -0.57989        0.727
            51          -0.57670        0.727
            52          -0.57361        0.727
            53          -0.57063        0.729
            54          -0.56773        0.729
            55          -0.56492        0.731
            56          -0.56220        0.734
            57          -0.55955        0.734
            58          -0.55699        0.734
            59          -0.55449        0.734
            60          -0.55207        0.734
            61          -0.54971        0.734
            62          -0.54741        0.734
            63          -0.54518        0.734
            64          -0.54300        0.738
            65          -0.54088        0.738
            66          -0.53881        0.738
            67          -0.53680        0.740
            68          -0.53483        0.740
            69          -0.53291        0.740
            70          -0.53104        0.740
            71          -0.52921        0.740
            72          -0.52743        0.742
            73          -0.52568        0.745
            74          -0.52397        0.745
            75          -0.52230        0.745
            76          -0.52067        0.745
            77          -0.51907        0.745
            78          -0.51750        0.745
            79          -0.51597        0.745
            80          -0.51447        0.745
            81          -0.51300        0.745
            82          -0.51156        0.745
            83          -0.51015        0.745
            84          -0.50876        0.745
            85          -0.50741        0.745
            86          -0.50607        0.745
            87          -0.50477        0.745
            88          -0.50348        0.745
            89          -0.50222        0.745
            90          -0.50099        0.745
            91          -0.49977        0.745
            92          -0.49858        0.745
            93          -0.49741        0.747
            94          -0.49625        0.747
            95          -0.49512        0.747
            96          -0.49401        0.747
            97          -0.49291        0.747
            98          -0.49183        0.747
            99          -0.49077        0.747
         Final          -0.48973        0.747
0.24307692307692308
10.0
Most Informative Features
           count(('p',)) = None               fr : en     =     30.3 : 1.0
           count(('m',)) = None               fr : en     =     27.0 : 1.0
           count(('i',)) = None               pt : en     =     26.2 : 1.0
           count(('h',)) = None               pt : en     =     25.5 : 1.0
           count(('l',)) = None               pt : en     =     18.7 : 1.0
           count(('k',)) = 0                  en : pt     =     17.8 : 1.0
           count(('y',)) = 0                  en : pt     =     16.8 : 1.0
           count(('w',)) = 0                  en : pt     =     15.6 : 1.0
           count(('.',)) = None               fr : en     =     14.5 : 1.0
           count(('d',)) = None               pt : en     =     12.5 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.56
10.0
  -2.750 count(('y',))==0 and label is 'pt'
  -2.391 count(('"',))==0 and label is 'es'
  -2.305 count(('3',))==0 and label is 'en'
  -2.289 count(('"',))==0 and label is 'pt'
   1.672 count(('_',))==0 and label is 'es'
  -1.669 count(('7',))==0 and label is 'es'
   1.481 count(('+',))==0 and label is 'pt'
  -1.475 count(('k',))==0 and label is 'pt'
  -1.413 count(('-',))==0 and label is 'es'
  -1.403 count(('z',))==0 and label is 'fr'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.371
             2          -1.30128        0.371
             3          -1.24156        0.478
             4          -1.18613        0.588
             5          -1.13497        0.636
             6          -1.08793        0.654
             7          -1.04479        0.684
             8          -1.00526        0.713
             9          -0.96906        0.739
            10          -0.93588        0.746
            11          -0.90543        0.757
            12          -0.87747        0.757
            13          -0.85173        0.757
            14          -0.82799        0.757
            15          -0.80607        0.757
            16          -0.78577        0.761
            17          -0.76694        0.761
            18          -0.74944        0.761
            19          -0.73314        0.772
            20          -0.71792        0.776
            21          -0.70370        0.776
            22          -0.69037        0.783
            23          -0.67786        0.787
            24          -0.66610        0.787
            25          -0.65503        0.787
            26          -0.64459        0.787
            27          -0.63472        0.787
            28          -0.62538        0.787
            29          -0.61654        0.787
            30          -0.60814        0.787
            31          -0.60017        0.787
            32          -0.59258        0.787
            33          -0.58536        0.787
            34          -0.57847        0.787
            35          -0.57189        0.787
            36          -0.56560        0.790
            37          -0.55959        0.790
            38          -0.55383        0.790
            39          -0.54830        0.790
            40          -0.54301        0.790
            41          -0.53792        0.790
            42          -0.53302        0.790
            43          -0.52832        0.790
            44          -0.52379        0.790
            45          -0.51942        0.794
            46          -0.51521        0.794
            47          -0.51114        0.794
            48          -0.50722        0.798
            49          -0.50342        0.798
            50          -0.49976        0.798
            51          -0.49621        0.798
            52          -0.49277        0.798
            53          -0.48944        0.798
            54          -0.48621        0.801
            55          -0.48308        0.801
            56          -0.48004        0.801
            57          -0.47709        0.801
            58          -0.47423        0.801
            59          -0.47144        0.801
            60          -0.46873        0.801
            61          -0.46610        0.801
            62          -0.46353        0.801
            63          -0.46104        0.801
            64          -0.45860        0.805
            65          -0.45623        0.805
            66          -0.45392        0.805
            67          -0.45166        0.805
            68          -0.44946        0.805
            69          -0.44731        0.805
            70          -0.44522        0.805
            71          -0.44317        0.812
            72          -0.44117        0.812
            73          -0.43921        0.812
            74          -0.43730        0.812
            75          -0.43542        0.812
            76          -0.43359        0.812
            77          -0.43180        0.812
            78          -0.43004        0.812
            79          -0.42833        0.812
            80          -0.42664        0.816
            81          -0.42499        0.816
            82          -0.42337        0.816
            83          -0.42179        0.816
            84          -0.42023        0.816
            85          -0.41871        0.816
            86          -0.41721        0.816
            87          -0.41574        0.816
            88          -0.41430        0.816
            89          -0.41289        0.816
            90          -0.41150        0.816
            91          -0.41013        0.816
            92          -0.40879        0.816
            93          -0.40747        0.816
            94          -0.40618        0.816
            95          -0.40490        0.816
            96          -0.40365        0.816
            97          -0.40242        0.816
            98          -0.40121        0.816
            99          -0.40002        0.816
         Final          -0.39884        0.816
0.25280898876404495
10.0
Most Informative Features
           count(('h',)) = None               pt : en     =     20.1 : 1.0
           count(('.',)) = None               fr : en     =     16.4 : 1.0
           count(('k',)) = 0                  en : pt     =     16.4 : 1.0
           count(('w',)) = 0                  en : fr     =     14.1 : 1.0
           count(('l',)) = None               pt : en     =     13.7 : 1.0
           count(('d',)) = None               fr : en     =     13.6 : 1.0
           count(('y',)) = 0                  en : pt     =     13.1 : 1.0
           count(('m',)) = None               fr : en     =     12.8 : 1.0
           count((':',)) = 0                  es : pt     =     10.3 : 1.0
           count(('"',)) = 0                  en : pt     =      9.8 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.6853932584269663
33.0
  -2.829 count(('y',))==0 and label is 'pt'
  -2.699 count(('3',))==0 and label is 'en'
  -2.428 count(('-',))==0 and label is 'es'
  -2.399 count((':',))==0 and label is 'pt'
  -2.180 count(('"',))==0 and label is 'pt'
  -1.727 count(('w',))==0 and label is 'fr'
  -1.670 count(('"',))==0 and label is 'es'
  -1.668 count(('k',))==0 and label is 'pt'
   1.606 count(('+',))==0 and label is 'pt'
  -1.483 count(('7',))==0 and label is 'es'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.372
             2          -1.25963        0.372
             3          -1.20211        0.470
             4          -1.14850        0.557
             5          -1.09869        0.634
             6          -1.05254        0.672
             7          -1.00987        0.678
             8          -0.97045        0.699
             9          -0.93405        0.710
            10          -0.90044        0.727
            11          -0.86939        0.738
            12          -0.84066        0.749
            13          -0.81405        0.754
            14          -0.78938        0.760
            15          -0.76646        0.770
            16          -0.74514        0.787
            17          -0.72527        0.809
            18          -0.70673        0.814
            19          -0.68938        0.814
            20          -0.67314        0.814
            21          -0.65791        0.814
            22          -0.64359        0.814
            23          -0.63012        0.825
            24          -0.61742        0.825
            25          -0.60543        0.836
            26          -0.59410        0.836
            27          -0.58337        0.836
            28          -0.57320        0.836
            29          -0.56354        0.836
            30          -0.55437        0.836
            31          -0.54564        0.836
            32          -0.53732        0.836
            33          -0.52939        0.836
            34          -0.52181        0.836
            35          -0.51457        0.836
            36          -0.50765        0.842
            37          -0.50101        0.842
            38          -0.49465        0.847
            39          -0.48854        0.852
            40          -0.48268        0.852
            41          -0.47705        0.852
            42          -0.47163        0.852
            43          -0.46640        0.852
            44          -0.46137        0.852
            45          -0.45652        0.852
            46          -0.45184        0.852
            47          -0.44732        0.852
            48          -0.44295        0.858
            49          -0.43873        0.858
            50          -0.43464        0.858
            51          -0.43069        0.858
            52          -0.42685        0.858
            53          -0.42314        0.858
            54          -0.41953        0.858
            55          -0.41603        0.858
            56          -0.41264        0.858
            57          -0.40934        0.858
            58          -0.40613        0.858
            59          -0.40301        0.858
            60          -0.39998        0.858
            61          -0.39703        0.858
            62          -0.39415        0.858
            63          -0.39135        0.858
            64          -0.38862        0.863
            65          -0.38596        0.863
            66          -0.38337        0.863
            67          -0.38083        0.863
            68          -0.37836        0.869
            69          -0.37595        0.869
            70          -0.37359        0.869
            71          -0.37128        0.869
            72          -0.36903        0.869
            73          -0.36683        0.874
            74          -0.36468        0.874
            75          -0.36257        0.874
            76          -0.36050        0.874
            77          -0.35848        0.874
            78          -0.35651        0.874
            79          -0.35457        0.874
            80          -0.35267        0.874
            81          -0.35081        0.874
            82          -0.34898        0.874
            83          -0.34719        0.874
            84          -0.34543        0.874
            85          -0.34371        0.874
            86          -0.34202        0.874
            87          -0.34036        0.874
            88          -0.33873        0.874
            89          -0.33713        0.874
            90          -0.33556        0.874
            91          -0.33401        0.874
            92          -0.33249        0.880
            93          -0.33100        0.880
            94          -0.32953        0.880
            95          -0.32809        0.880
            96          -0.32667        0.880
            97          -0.32527        0.880
            98          -0.32390        0.880
            99          -0.32255        0.880
         Final          -0.32121        0.880
0.44
15.0
Most Informative Features
           count(('k',)) = 0                  en : pt     =     24.5 : 1.0
           count(('w',)) = None               pt : en     =     18.4 : 1.0
           count(('3',)) = None               en : fr     =     13.9 : 1.0
           count(('"',)) = 0                  en : es     =     10.3 : 1.0
           count(('y',)) = 0                  en : pt     =      9.1 : 1.0
           count(('w',)) = 0                  en : pt     =      8.8 : 1.0
           count(('k',)) = None               pt : en     =      8.5 : 1.0
           count(('9',)) = None               en : fr     =      7.1 : 1.0
           count((',',)) = None               pt : en     =      5.8 : 1.0
           count(('8',)) = 0                  fr : pt     =      5.6 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.712
34.0
  -3.036 count(('"',))==0 and label is 'es'
  -2.654 count(('k',))==0 and label is 'pt'
  -2.615 count(('y',))==0 and label is 'pt'
   2.338 count(('_',))==0 and label is 'es'
  -1.871 count(('3',))==0 and label is 'en'
  -1.855 count(('z',))==0 and label is 'fr'
  -1.750 count(('"',))==0 and label is 'pt'
   1.556 count(('+',))==0 and label is 'pt'
   1.522 count(('=',))==0 and label is 'fr'
  -1.420 count(('5',))==0 and label is 'es'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.373
             2          -1.25412        0.373
             3          -1.20107        0.455
             4          -1.15154        0.545
             5          -1.10532        0.555
             6          -1.06221        0.600
             7          -1.02201        0.673
             8          -0.98451        0.691
             9          -0.94952        0.745
            10          -0.91684        0.764
            11          -0.88630        0.773
            12          -0.85772        0.809
            13          -0.83095        0.818
            14          -0.80584        0.827
            15          -0.78227        0.836
            16          -0.76011        0.845
            17          -0.73925        0.855
            18          -0.71959        0.864
            19          -0.70104        0.864
            20          -0.68351        0.864
            21          -0.66693        0.864
            22          -0.65122        0.864
            23          -0.63634        0.864
            24          -0.62221        0.864
            25          -0.60878        0.864
            26          -0.59601        0.864
            27          -0.58385        0.864
            28          -0.57226        0.873
            29          -0.56121        0.873
            30          -0.55065        0.873
            31          -0.54057        0.873
            32          -0.53092        0.873
            33          -0.52168        0.873
            34          -0.51283        0.873
            35          -0.50434        0.882
            36          -0.49619        0.882
            37          -0.48837        0.882
            38          -0.48086        0.891
            39          -0.47363        0.900
            40          -0.46667        0.909
            41          -0.45997        0.909
            42          -0.45351        0.918
            43          -0.44729        0.918
            44          -0.44128        0.918
            45          -0.43548        0.927
            46          -0.42988        0.927
            47          -0.42447        0.927
            48          -0.41923        0.927
            49          -0.41417        0.936
            50          -0.40926        0.936
            51          -0.40452        0.936
            52          -0.39991        0.936
            53          -0.39545        0.936
            54          -0.39113        0.936
            55          -0.38693        0.936
            56          -0.38286        0.936
            57          -0.37890        0.936
            58          -0.37505        0.936
            59          -0.37132        0.936
            60          -0.36768        0.936
            61          -0.36415        0.936
            62          -0.36071        0.936
            63          -0.35736        0.936
            64          -0.35410        0.936
            65          -0.35092        0.945
            66          -0.34783        0.945
            67          -0.34481        0.945
            68          -0.34187        0.945
            69          -0.33900        0.955
            70          -0.33620        0.955
            71          -0.33347        0.955
            72          -0.33080        0.955
            73          -0.32819        0.955
            74          -0.32565        0.955
            75          -0.32316        0.955
            76          -0.32073        0.955
            77          -0.31835        0.945
            78          -0.31603        0.945
            79          -0.31375        0.945
            80          -0.31152        0.945
            81          -0.30935        0.945
            82          -0.30721        0.945
            83          -0.30512        0.945
            84          -0.30308        0.945
            85          -0.30108        0.945
            86          -0.29911        0.945
            87          -0.29719        0.945
            88          -0.29530        0.945
            89          -0.29345        0.945
            90          -0.29164        0.945
            91          -0.28986        0.945
            92          -0.28811        0.945
            93          -0.28640        0.945
            94          -0.28471        0.945
            95          -0.28306        0.945
            96          -0.28144        0.945
            97          -0.27985        0.945
            98          -0.27828        0.945
            99          -0.27675        0.945
         Final          -0.27523        0.945
0.44047619047619047
15.0
Most Informative Features
           count(('"',)) = None               pt : en     =     11.0 : 1.0
           count(('k',)) = 0                  en : pt     =      9.1 : 1.0
           count(('8',)) = None               pt : fr     =      9.0 : 1.0
           count(('2',)) = None               es : fr     =      8.6 : 1.0
           count(('y',)) = None               pt : es     =      8.0 : 1.0
           count(('0',)) = None               pt : fr     =      7.6 : 1.0
           count(('y',)) = 0                  en : pt     =      7.4 : 1.0
           count(('"',)) = 0                  en : pt     =      7.0 : 1.0
           count(('(',)) = None               pt : en     =      5.6 : 1.0
           count((')',)) = None               pt : en     =      5.6 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.7380952380952381
33.0
  -2.739 count(('y',))==0 and label is 'pt'
  -2.501 count(('"',))==0 and label is 'es'
   1.948 count(('_',))==0 and label is 'es'
  -1.873 count(('7',))==0 and label is 'es'
  -1.772 count(('"',))==0 and label is 'pt'
  -1.748 count(('-',))==0 and label is 'es'
  -1.660 count(('3',))==0 and label is 'en'
  -1.511 count(('k',))==0 and label is 'pt'
  -1.105 count(('5',))==0 and label is 'es'
   1.105 count(('?',))==0 and label is 'en'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.375
             2          -1.26602        0.375
             3          -1.23454        0.375
             4          -1.20436        0.375
             5          -1.17543        0.446
             6          -1.14771        0.500
             7          -1.12114        0.589
             8          -1.09570        0.589
             9          -1.07132        0.625
            10          -1.04798        0.643
            11          -1.02561        0.696
            12          -1.00417        0.732
            13          -0.98362        0.750
            14          -0.96392        0.750
            15          -0.94502        0.750
            16          -0.92688        0.750
            17          -0.90947        0.750
            18          -0.89275        0.750
            19          -0.87667        0.768
            20          -0.86122        0.768
            21          -0.84636        0.768
            22          -0.83205        0.768
            23          -0.81827        0.768
            24          -0.80500        0.768
            25          -0.79220        0.804
            26          -0.77986        0.804
            27          -0.76795        0.804
            28          -0.75645        0.804
            29          -0.74534        0.821
            30          -0.73460        0.821
            31          -0.72422        0.839
            32          -0.71418        0.839
            33          -0.70446        0.839
            34          -0.69505        0.839
            35          -0.68593        0.839
            36          -0.67709        0.839
            37          -0.66852        0.839
            38          -0.66021        0.839
            39          -0.65214        0.875
            40          -0.64430        0.875
            41          -0.63669        0.875
            42          -0.62930        0.875
            43          -0.62211        0.875
            44          -0.61512        0.875
            45          -0.60832        0.875
            46          -0.60170        0.875
            47          -0.59526        0.875
            48          -0.58898        0.875
            49          -0.58287        0.875
            50          -0.57691        0.875
            51          -0.57110        0.875
            52          -0.56543        0.875
            53          -0.55991        0.875
            54          -0.55451        0.875
            55          -0.54925        0.875
            56          -0.54411        0.875
            57          -0.53909        0.875
            58          -0.53418        0.875
            59          -0.52939        0.875
            60          -0.52470        0.875
            61          -0.52012        0.875
            62          -0.51563        0.875
            63          -0.51125        0.893
            64          -0.50696        0.893
            65          -0.50276        0.893
            66          -0.49864        0.893
            67          -0.49461        0.893
            68          -0.49067        0.911
            69          -0.48680        0.911
            70          -0.48301        0.911
            71          -0.47930        0.911
            72          -0.47566        0.911
            73          -0.47209        0.911
            74          -0.46858        0.911
            75          -0.46515        0.911
            76          -0.46178        0.911
            77          -0.45847        0.911
            78          -0.45522        0.911
            79          -0.45203        0.911
            80          -0.44890        0.911
            81          -0.44582        0.911
            82          -0.44280        0.911
            83          -0.43983        0.911
            84          -0.43691        0.911
            85          -0.43404        0.911
            86          -0.43122        0.911
            87          -0.42845        0.911
            88          -0.42572        0.911
            89          -0.42304        0.911
            90          -0.42040        0.911
            91          -0.41781        0.911
            92          -0.41526        0.911
            93          -0.41274        0.911
            94          -0.41027        0.911
            95          -0.40783        0.911
            96          -0.40544        0.911
            97          -0.40307        0.911
            98          -0.40075        0.911
            99          -0.39846        0.911
         Final          -0.39620        0.911
0.3620689655172414
15.0
Most Informative Features
           count(('y',)) = None               pt : fr     =      7.0 : 1.0
           count(('z',)) = None               fr : pt     =      6.9 : 1.0
           count(('k',)) = 0                  en : pt     =      6.0 : 1.0
           count(('-',)) = None               es : en     =      4.2 : 1.0
           count(('5',)) = None               es : pt     =      3.9 : 1.0
           count(('"',)) = 0                  fr : pt     =      3.9 : 1.0
           count((':',)) = None               pt : fr     =      3.6 : 1.0
           count(('j',)) = None               es : fr     =      3.1 : 1.0
           count(('4',)) = None               pt : fr     =      3.1 : 1.0
           count(('6',)) = None               pt : fr     =      3.0 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.4827586206896552
19.0
   1.932 count(('_',))==0 and label is 'es'
  -1.850 count(('k',))==0 and label is 'pt'
  -1.780 count(('"',))==0 and label is 'es'
   1.539 count(('^',))==0 and label is 'en'
  -1.359 count(('y',))==0 and label is 'pt'
  -1.324 count(('-',))==0 and label is 'es'
   1.307 count(('=',))==0 and label is 'en'
  -1.285 count(('"',))==0 and label is 'pt'
   1.197 count(('%',))==0 and label is 'fr'
  -1.182 count(('7',))==0 and label is 'es'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.373
             2          -1.39423        0.269
             3          -1.37167        0.269
             4          -1.35515        0.271
             5          -1.34222        0.327
             6          -1.33086        0.328
             7          -1.32073        0.330
             8          -1.31198        0.328
             9          -1.30449        0.330
            10          -1.29807        0.330
            11          -1.29253        0.330
            12          -1.28772        0.332
            13          -1.28351        0.332
            14          -1.27980        0.332
            15          -1.27652        0.332
            16          -1.27360        0.332
            17          -1.27099        0.332
            18          -1.26864        0.330
            19          -1.26652        0.330
            20          -1.26460        0.330
            21          -1.26285        0.330
            22          -1.26125        0.330
            23          -1.25979        0.332
            24          -1.25844        0.330
            25          -1.25720        0.330
            26          -1.25605        0.330
            27          -1.25499        0.332
            28          -1.25401        0.332
            29          -1.25309        0.334
            30          -1.25224        0.334
            31          -1.25144        0.334
            32          -1.25069        0.334
            33          -1.24999        0.334
            34          -1.24933        0.334
            35          -1.24871        0.334
            36          -1.24811        0.334
            37          -1.24755        0.334
            38          -1.24702        0.334
            39          -1.24650        0.334
            40          -1.24601        0.334
            41          -1.24554        0.334
            42          -1.24508        0.334
            43          -1.24465        0.334
            44          -1.24422        0.334
            45          -1.24381        0.334
            46          -1.24342        0.334
            47          -1.24303        0.334
            48          -1.24266        0.334
            49          -1.24229        0.334
            50          -1.24194        0.334
            51          -1.24160        0.334
            52          -1.24126        0.334
            53          -1.24093        0.334
            54          -1.24061        0.334
            55          -1.24029        0.334
            56          -1.23999        0.334
            57          -1.23969        0.334
            58          -1.23939        0.334
            59          -1.23910        0.334
            60          -1.23882        0.334
            61          -1.23854        0.334
            62          -1.23827        0.332
            63          -1.23800        0.332
            64          -1.23773        0.332
            65          -1.23747        0.332
            66          -1.23722        0.332
            67          -1.23697        0.332
            68          -1.23672        0.332
            69          -1.23648        0.332
            70          -1.23624        0.332
            71          -1.23601        0.332
            72          -1.23577        0.332
            73          -1.23555        0.332
            74          -1.23532        0.332
            75          -1.23510        0.332
            76          -1.23488        0.332
            77          -1.23466        0.332
         Final               nan        0.332
0.4
10.0
Most Informative Features
       count(('g', ' ')) = 0                  en : pt     =     96.6 : 1.0
       count(('t', 'h')) = 0                  en : pt     =     78.9 : 1.0
       count(('h', ' ')) = 0                  en : fr     =     68.4 : 1.0
       count(('t', 's')) = 0                  en : pt     =     61.6 : 1.0
       count(('m', 'm')) = 0                  en : pt     =     61.6 : 1.0
       count(('f', ' ')) = 0                  en : es     =     59.5 : 1.0
      count(('\\', 'r')) = None               pt : en     =     59.2 : 1.0
       count(('o', 'w')) = 0                  en : pt     =     55.5 : 1.0
       count(('a', 'y')) = 0                  en : pt     =     52.5 : 1.0
      count(('r', '\\')) = None               pt : en     =     48.4 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.26153846153846155
10.0
  34.218 count(("'", 'r'))==0 and label is 'pt'
  32.024 count(('x', 'o'))==0 and label is 'en'
  27.550 count(('b', 'j'))==0 and label is 'en'
  26.860 count(('l', 'f'))==0 and label is 'en'
  26.854 count(('c', 'y'))==0 and label is 'en'
  26.131 count(('n', 'w'))==0 and label is 'en'
  26.107 count(('6', '1'))==0 and label is 'en'
  24.162 count(('"', 'f'))==0 and label is 'en'
  22.388 count(('(', '4'))==0 and label is 'es'
  22.387 count(('e', 'h'))==0 and label is 'es'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.371
             2          -1.43740        0.268
             3          -1.41493        0.301
             4          -1.39683        0.301
             5          -1.38191        0.298
             6          -1.36934        0.305
             7          -1.35905        0.309
             8          -1.35071        0.309
             9          -1.34396        0.309
            10          -1.33850        0.312
            11          -1.33410        0.312
            12          -1.33058        0.312
            13          -1.32778        0.312
            14          -1.32558        0.312
            15          -1.32389        0.312
            16          -1.32260        0.312
            17          -1.32166        0.312
            18          -1.32100        0.312
            19          -1.32058        0.309
            20          -1.32035        0.309
            21          -1.32028        0.309
            22          -1.32034        0.309
            23          -1.32051        0.309
            24          -1.32076        0.309
            25          -1.32109        0.309
            26          -1.32147        0.309
            27          -1.32190        0.309
            28          -1.32235        0.309
            29          -1.32284        0.309
            30          -1.32333        0.309
            31          -1.32384        0.309
            32          -1.32436        0.309
            33          -1.32488        0.309
            34          -1.32539        0.309
            35          -1.32590        0.309
            36          -1.32640        0.309
            37          -1.32688        0.309
            38          -1.32736        0.309
            39          -1.32782        0.309
            40          -1.32826        0.309
            41          -1.32869        0.309
            42          -1.32909        0.309
            43          -1.32948        0.309
            44          -1.32984        0.309
            45          -1.33019        0.309
            46          -1.33051        0.312
            47          -1.33081        0.312
            48          -1.33110        0.312
         Final               nan        0.320
0.5224719101123596
20.0
Most Informative Features
       count((' ', 'w')) = 0                  en : pt     =     52.1 : 1.0
       count(('l', 'l')) = 0                  en : pt     =     46.1 : 1.0
       count(('h', ' ')) = 0                  en : pt     =     44.6 : 1.0
       count(('o', 'f')) = 0                  en : fr     =     40.0 : 1.0
       count(('o', ' ')) = 0                  en : fr     =     37.8 : 1.0
       count(('o', 'w')) = 0                  en : pt     =     31.0 : 1.0
       count(('t', 'h')) = None               pt : en     =     29.9 : 1.0
       count(('t', 's')) = 0                  en : pt     =     29.5 : 1.0
       count(('f', ' ')) = 0                  en : es     =     28.9 : 1.0
       count(('e', 'e')) = 0                  en : pt     =     28.0 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.24157303370786518
9.0
  19.134 count(("'", 'r'))==0 and label is 'en'
  16.291 count(("'", 'w'))==0 and label is 'en'
  16.291 count(('r', 'h'))==0 and label is 'en'
  15.479 count(('g', 'y'))==0 and label is 'en'
  15.422 count(('a', ')'))==0 and label is 'en'
  15.052 count(('"', 'i'))==0 and label is 'en'
  14.868 count(('c', ':'))==0 and label is 'en'
  14.761 count(("'", 'l'))==0 and label is 'en'
  13.413 count(('o', ')'))==0 and label is 'en'
  13.393 count(('(', 'n'))==0 and label is 'en'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.372
             2          -1.30964        0.268
             3          -1.29464        0.268
             4          -1.28197        0.268
             5          -1.27491        0.268
             6          -1.27088        0.268
             7          -1.26838        0.268
             8          -1.26668        0.273
             9          -1.26545        0.273
            10          -1.26452        0.273
            11          -1.26379        0.273
            12          -1.26322        0.273
            13          -1.26276        0.273
            14          -1.26240        0.273
            15          -1.26210        0.273
            16          -1.26187        0.273
            17          -1.26169        0.273
            18          -1.26155        0.273
            19          -1.26146        0.273
            20          -1.26139        0.273
            21          -1.26136        0.273
            22          -1.26136        0.273
            23          -1.26138        0.273
            24          -1.26142        0.273
            25          -1.26147        0.273
            26          -1.26155        0.273
            27          -1.26163        0.273
            28          -1.26173        0.273
            29          -1.26183        0.273
            30          -1.26194        0.273
            31          -1.26206        0.273
            32          -1.26219        0.273
            33          -1.26232        0.273
            34          -1.26245        0.273
            35          -1.26258        0.273
            36          -1.26272        0.273
            37          -1.26285        0.273
            38          -1.26299        0.273
            39          -1.26312        0.273
            40          -1.26326        0.273
            41          -1.26339        0.273
            42          -1.26353        0.273
            43          -1.26366        0.273
            44          -1.26379        0.273
            45          -1.26391        0.273
            46          -1.26404        0.279
            47          -1.26416        0.279
            48          -1.26429        0.279
            49          -1.26441        0.279
            50          -1.26452        0.279
            51          -1.26464        0.279
            52          -1.26475        0.279
            53          -1.26486        0.279
            54          -1.26497        0.279
            55          -1.26508        0.279
            56          -1.26518        0.279
            57          -1.26528        0.279
            58          -1.26538        0.279
            59          -1.26548        0.279
            60          -1.26558        0.279
            61          -1.26567        0.279
            62          -1.26577        0.279
            63          -1.26586        0.279
            64          -1.26595        0.279
            65          -1.26604        0.279
            66          -1.26612        0.279
            67          -1.26621        0.279
            68          -1.26629        0.279
            69          -1.26637        0.279
            70          -1.26645        0.279
            71          -1.26653        0.279
            72          -1.26661        0.279
            73          -1.26669        0.279
            74          -1.26676        0.279
            75          -1.26683        0.279
            76          -1.26691        0.279
            77          -1.26698        0.279
            78          -1.26705        0.279
            79          -1.26712        0.279
            80          -1.26719        0.279
            81          -1.26725        0.279
            82          -1.26732        0.279
            83          -1.26739        0.279
            84          -1.26745        0.279
            85          -1.26751        0.279
            86          -1.26757        0.279
            87          -1.26763        0.279
            88          -1.26769        0.279
            89          -1.26775        0.273
            90          -1.26781        0.273
            91          -1.26786        0.273
            92          -1.26792        0.273
            93          -1.26797        0.273
            94          -1.26802        0.273
            95          -1.26807        0.273
            96          -1.26812        0.273
            97          -1.26817        0.273
            98          -1.26822        0.273
            99          -1.26826        0.273
         Final          -1.26831        0.273
0.728
28.0
Most Informative Features
       count(('t', 'h')) = 0                  en : pt     =     45.3 : 1.0
       count(('g', ' ')) = 0                  en : pt     =     42.3 : 1.0
       count(('l', 'l')) = 0                  en : pt     =     42.3 : 1.0
       count((' ', 'w')) = 0                  en : pt     =     40.8 : 1.0
       count(('h', ' ')) = 0                  en : pt     =     40.8 : 1.0
       count(('t', 's')) = 0                  en : pt     =     31.9 : 1.0
       count(('e', 'd')) = 0                  en : fr     =     31.7 : 1.0
       count(('w', 'i')) = 0                  en : pt     =     28.9 : 1.0
       count(('t', ' ')) = 0                  en : pt     =     27.2 : 1.0
       count(('k', 'e')) = 0                  en : pt     =     24.5 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.288
12.0
  26.065 count(('b', 'j'))==0 and label is 'en'
  25.094 count(('3', ','))==0 and label is 'en'
  23.022 count(("'", 'r'))==0 and label is 'pt'
  20.402 count(('1', 'v'))==0 and label is 'pt'
  18.705 count(('1', 'f'))==0 and label is 'pt'
  18.598 count(('.', "'"))==0 and label is 'en'
  18.100 count(('(', '4'))==0 and label is 'es'
  18.100 count(('4', ')'))==0 and label is 'es'
  18.079 count(('u', 'o'))==0 and label is 'pt'
  17.543 count(('9', 'p'))==0 and label is 'pt'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.373
             2          -1.15311        0.300
             3          -1.18528        0.300
             4          -1.19365        0.300
             5          -1.19577        0.300
             6          -1.19627        0.300
             7          -1.19620        0.300
             8          -1.19571        0.300
             9          -1.19484        0.300
            10          -1.19358        0.300
            11          -1.19215        0.300
            12          -1.19067        0.300
            13          -1.18918        0.300
            14          -1.18771        0.309
            15          -1.18629        0.309
            16          -1.18495        0.309
            17          -1.18370        0.309
            18          -1.18255        0.309
            19          -1.18151        0.309
            20          -1.18057        0.309
            21          -1.17974        0.309
            22          -1.17900        0.309
            23          -1.17835        0.309
            24          -1.17778        0.309
            25          -1.17728        0.309
            26          -1.17685        0.309
            27          -1.17648        0.309
            28          -1.17616        0.309
            29          -1.17588        0.309
            30          -1.17564        0.309
            31          -1.17544        0.309
            32          -1.17526        0.309
            33          -1.17510        0.309
            34          -1.17497        0.309
            35          -1.17486        0.309
            36          -1.17476        0.309
            37          -1.17467        0.309
            38          -1.17460        0.309
            39          -1.17453        0.309
            40          -1.17448        0.309
            41          -1.17443        0.309
            42          -1.17439        0.309
            43          -1.17435        0.309
            44          -1.17432        0.309
            45          -1.17429        0.309
            46          -1.17427        0.309
            47          -1.17425        0.309
            48          -1.17423        0.309
            49          -1.17422        0.309
            50          -1.17420        0.309
            51          -1.17419        0.309
            52          -1.17418        0.309
            53          -1.17417        0.309
            54          -1.17417        0.309
            55          -1.17416        0.309
         Final               nan        0.300
0.7857142857142857
29.0
Most Informative Features
       count(('t', 'h')) = 0                  en : pt     =     27.3 : 1.0
       count((' ', 'w')) = 0                  en : pt     =     27.3 : 1.0
       count(('"', ' ')) = 0                  en : pt     =     25.8 : 1.0
       count(('m', 'm')) = 0                  fr : pt     =     24.7 : 1.0
       count(('w', 'i')) = 0                  en : pt     =     22.8 : 1.0
       count(('b', 'y')) = 0                  en : pt     =     22.8 : 1.0
       count(('k', 'e')) = 0                  en : pt     =     21.4 : 1.0
       count(('9', '9')) = 0                  fr : pt     =     20.1 : 1.0
       count(('x', ' ')) = 0                  fr : pt     =     20.1 : 1.0
       count(('u', 'v')) = 0                  fr : pt     =     20.1 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.2619047619047619
9.0
  27.968 count(('9', ','))==0 and label is 'pt'
  26.447 count(('z', ','))==0 and label is 'pt'
  25.579 count(("'", 'b'))==0 and label is 'pt'
  24.669 count(('(', '4'))==0 and label is 'es'
  24.655 count(('4', ')'))==0 and label is 'es'
  24.619 count(('o', '\\'))==0 and label is 'es'
  22.923 count(('2', 'm'))==0 and label is 'pt'
  22.923 count((',', 'o'))==0 and label is 'pt'
  22.507 count(('7', '0'))==0 and label is 'pt'
  20.491 count(('3', 'p'))==0 and label is 'pt'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.375
             2          -0.85342        0.429
             3          -0.84841        0.429
             4          -0.84746        0.429
             5          -0.84732        0.429
             6          -0.84730        0.429
             7          -0.84730        0.429
             8          -0.84730        0.429
             9          -0.84730        0.429
            10          -0.84730        0.429
            11          -0.84730        0.429
            12          -0.84730        0.429
            13          -0.84730        0.429
            14          -0.84730        0.429
            15          -0.84730        0.429
            16          -0.84730        0.429
            17          -0.84730        0.429
            18          -0.84730        0.429
            19          -0.84730        0.429
            20          -0.84730        0.429
            21          -0.84730        0.429
            22          -0.84730        0.429
            23          -0.84730        0.429
            24          -0.84730        0.429
            25          -0.84730        0.429
            26          -0.84730        0.429
            27          -0.84730        0.429
            28          -0.84730        0.429
            29          -0.84730        0.429
            30          -0.84730        0.429
            31          -0.84730        0.429
            32          -0.84730        0.429
            33          -0.84730        0.429
            34          -0.84730        0.429
            35          -0.84730        0.429
            36          -0.84730        0.429
            37          -0.84730        0.429
            38          -0.84730        0.429
            39          -0.84730        0.429
            40          -0.84730        0.429
            41          -0.84730        0.429
         Final               nan        0.321
0.8620689655172413
33.0
Most Informative Features
       count(('m', 'm')) = 0                  fr : pt     =     14.2 : 1.0
       count(('3', 'o')) = None               fr : pt     =     14.2 : 1.0
       count(('a', '1')) = None               fr : pt     =     14.2 : 1.0
       count(('"', ' ')) = 0                  fr : pt     =     14.2 : 1.0
       count(('a', '3')) = None               fr : pt     =     14.2 : 1.0
       count(('e', 'e')) = 0                  en : pt     =     13.9 : 1.0
      count(('m', '\\')) = None               en : pt     =     13.9 : 1.0
       count(('w', 'i')) = 0                  en : pt     =     13.9 : 1.0
       count(('t', 's')) = 0                  en : pt     =     13.9 : 1.0
       count(('o', 'w')) = 0                  en : pt     =     13.9 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.39655172413793105
14.0
  18.391 count(('9', 'v'))==0 and label is 'pt'
  15.720 count(('5', ' '))==0 and label is 'es'
  15.701 count(('1', '5'))==0 and label is 'es'
  15.701 count(('b', 't'))==0 and label is 'es'
  15.696 count(('y', 'a'))==0 and label is 'es'
  15.671 count(('/', 'o'))==0 and label is 'es'
  15.637 count(('3', 'm'))==0 and label is 'es'
  15.601 count(('y', '/'))==0 and label is 'es'
  15.600 count(('b', '.'))==0 and label is 'es'
  14.686 count(('m', ')'))==0 and label is 'pt'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.373
             2          -0.77440        0.518
             3          -0.83332        0.443
             4          -0.83580        0.496
             5          -0.83142        0.498
             6          -0.82888        0.498
             7          -0.82583        0.498
             8          -0.82319        0.496
         Final               nan        0.465
0.6923076923076923
10.0
Most Informative Features
  count((' ', 'o', 'f')) = 0                  en : pt     =    113.3 : 1.0
  count(('n', 'd', ' ')) = 0                  en : pt     =     99.6 : 1.0
  count(('o', 'f', ' ')) = 0                  en : fr     =     79.3 : 1.0
  count(('a', 'n', ' ')) = 0                  en : pt     =     73.7 : 1.0
  count(('a', 'l', 'l')) = 0                  en : pt     =     73.7 : 1.0
  count(('h', 'e', ' ')) = 0                  en : pt     =     73.4 : 1.0
  count(('t', 'o', ' ')) = 0                  en : fr     =     71.6 : 1.0
 count(('r', '\\', 'n')) = 0                  en : es     =     70.8 : 1.0
  count(('e', 'd', ' ')) = 0                  en : pt     =     65.2 : 1.0
  count(('u', 't', 'e')) = 0                  en : pt     =     62.5 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.38153846153846155
10.0
   5.108 count(('n', ' ', 'o'))==0 and label is 'fr'
   5.108 count((' ', 'o', 'r'))==0 and label is 'fr'
   5.108 count(('r', ' ', 'e'))==0 and label is 'fr'
   5.108 count(('i', ' ', 'f'))==0 and label is 'fr'
   5.108 count(('n', 'c', 't'))==0 and label is 'fr'
   5.108 count(('o', 'n', 'n'))==0 and label is 'fr'
   5.108 count(('n', 'n', 'e'))==0 and label is 'fr'
   5.108 count(('a', ' ', 'l'))==0 and label is 'fr'
   5.108 count(('u', 'r', 'e'))==0 and label is 'fr'
   5.108 count((' ', 'd', "'"))==0 and label is 'fr'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.371
             2          -0.57725        0.618
             3          -0.58053        0.618
             4          -0.57779        0.618
             5          -0.57536        0.618
             6          -0.57429        0.618
             7          -0.57352        0.614
             8          -0.57288        0.614
             9          -0.57253        0.614
            10          -0.57228        0.610
            11          -0.57207        0.610
         Final               nan        0.548
0.7247191011235955
30.0
Most Informative Features
  count((' ', 'o', 'f')) = 0                  en : pt     =     53.6 : 1.0
  count(('n', 'd', ' ')) = 0                  en : pt     =     52.1 : 1.0
 count(('r', '\\', 'n')) = 0                  en : fr     =     48.8 : 1.0
  count(('i', 'n', 'g')) = 0                  en : pt     =     47.6 : 1.0
  count(('b', 'e', ' ')) = 0                  en : pt     =     38.5 : 1.0
  count(('h', 'e', ' ')) = 0                  en : pt     =     37.6 : 1.0
  count(('e', 'd', ' ')) = 0                  en : fr     =     36.7 : 1.0
  count(('a', 'l', 'l')) = 0                  en : pt     =     34.0 : 1.0
  count(('s', 't', ' ')) = 0                  en : pt     =     32.5 : 1.0
  count(('c', 't', 'i')) = 0                  en : pt     =     31.0 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.48314606741573035
20.0
   7.024 count(('u', 'n', ' '))==0 and label is 'fr'
   7.024 count(('n', ' ', 'o'))==0 and label is 'fr'
   7.024 count((' ', 'o', 'r'))==0 and label is 'fr'
   7.024 count(('o', 'r', 'd'))==0 and label is 'fr'
   7.024 count(('r', 'd', 'i'))==0 and label is 'fr'
   7.024 count(('d', 'i', 'n'))==0 and label is 'fr'
   7.024 count(('i', 'n', 'a'))==0 and label is 'fr'
   7.024 count(('n', 'a', 't'))==0 and label is 'fr'
   7.024 count(('a', 't', 'e'))==0 and label is 'fr'
   7.024 count(('t', 'e', 'u'))==0 and label is 'fr'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.372
             2          -0.62375        0.530
             3          -0.68779        0.492
             4          -0.70816        0.486
             5          -0.71398        0.486
             6          -0.71689        0.486
             7          -0.71908        0.486
             8          -0.72106        0.481
         Final               nan        0.448
0.976
40.0
Most Informative Features
  count(('h', 'e', ' ')) = 0                  en : pt     =     45.3 : 1.0
  count((' ', 'o', 'f')) = 0                  en : pt     =     42.3 : 1.0
  count((' ', 'b', 'e')) = 0                  en : pt     =     40.8 : 1.0
  count(('n', 'd', ' ')) = 0                  en : pt     =     40.8 : 1.0
  count(('u', 't', 'e')) = 0                  en : pt     =     39.3 : 1.0
  count(('b', 'e', ' ')) = 0                  en : pt     =     34.9 : 1.0
  count((' ', 'o', 'r')) = 0                  en : pt     =     33.4 : 1.0
  count(('d', ' ', 'a')) = 0                  en : pt     =     33.4 : 1.0
  count(('t', 'o', ' ')) = 0                  en : fr     =     32.8 : 1.0
  count(('e', 'd', ' ')) = 0                  en : fr     =     31.7 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.552
19.0
   5.087 count(('a', ' ', 'c'))==0 and label is 'en'
   5.087 count((' ', 'c', 'o'))==0 and label is 'en'
   5.087 count(('c', 'o', 'm'))==0 and label is 'en'
   5.087 count(('o', 'm', 'p'))==0 and label is 'en'
   5.087 count(('m', 'p', 'u'))==0 and label is 'en'
   5.087 count(('p', 'u', 't'))==0 and label is 'en'
   5.087 count(('u', 't', 'e'))==0 and label is 'en'
   5.087 count(('t', 'e', 'r'))==0 and label is 'en'
   5.087 count(('e', 'r', ' '))==0 and label is 'en'
   5.087 count((' ', 'i', 's'))==0 and label is 'en'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.373
         Final               nan        0.164
1.0
40.0
Most Informative Features
 count(('\\', 'x', 'a')) = None               en : pt     =     27.3 : 1.0
  count(('h', 'e', ' ')) = 0                  en : pt     =     27.3 : 1.0
 count(('.', '\\', 'n')) = None               en : pt     =     27.3 : 1.0
  count((' ', 'o', 'f')) = 0                  en : pt     =     27.3 : 1.0
  count(('n', 'e', ' ')) = 0                  fr : pt     =     26.6 : 1.0
  count(('l', 'l', 'y')) = 0                  en : pt     =     25.8 : 1.0
  count(('a', 'n', ' ')) = 0                  en : pt     =     25.8 : 1.0
  count(('a', 'l', 'l')) = 0                  en : pt     =     25.8 : 1.0
  count(('e', 't', ' ')) = 0                  fr : pt     =     25.7 : 1.0
  count((' ', 'e', 't')) = 0                  fr : pt     =     25.7 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.30952380952380953
10.0
     nan count(('b', "'", 'a'))==0 and label is 'en'
     nan count(("'", 'a', ' '))==0 and label is 'en'
     nan count(('a', ' ', 'c'))==0 and label is 'en'
     nan count((' ', 'c', 'o'))==0 and label is 'en'
     nan count(('c', 'o', 'm'))==0 and label is 'en'
     nan count(('o', 'm', 'p'))==0 and label is 'en'
     nan count(('m', 'p', 'u'))==0 and label is 'en'
     nan count(('p', 'u', 't'))==0 and label is 'en'
     nan count(('u', 't', 'e'))==0 and label is 'en'
     nan count(('t', 'e', 'r'))==0 and label is 'en'
-----------------------------------------------hh--------------------------------------
  ==> Training (100 iterations)

      Iteration    Log Likelihood    Accuracy
      ---------------------------------------
             1          -1.38629        0.375
         Final               nan        0.161
1.0
40.0
Most Informative Features
  count(('3', 'o', ' ')) = None               fr : pt     =     14.2 : 1.0
  count(('d', 'a', 'd')) = None               fr : pt     =     14.2 : 1.0
  count(('e', 'n', ' ')) = 0                  fr : pt     =     14.2 : 1.0
  count(('x', 'a', '1')) = None               fr : pt     =     14.2 : 1.0
  count(('x', 'a', 'd')) = None               fr : pt     =     14.2 : 1.0
  count(('n', 'a', 't')) = 0                  fr : pt     =     14.2 : 1.0
  count(('n', ' ', 'd')) = 0                  fr : pt     =     14.2 : 1.0
  count(('x', 'a', '3')) = None               fr : pt     =     14.2 : 1.0
  count(('n', 'e', ' ')) = 0                  fr : pt     =     14.2 : 1.0
  count(('a', '3', 'o')) = None               fr : pt     =     14.2 : 1.0
------------------------------LOGISTIC REGRESSION-------------------------------------
0.29310344827586204
10.0
     nan count(('b', "'", 'a'))==0 and label is 'en'
     nan count(("'", 'a', ' '))==0 and label is 'en'
     nan count(('a', ' ', 'c'))==0 and label is 'en'
     nan count((' ', 'c', 'o'))==0 and label is 'en'
     nan count(('c', 'o', 'm'))==0 and label is 'en'
     nan count(('o', 'm', 'p'))==0 and label is 'en'
     nan count(('m', 'p', 'u'))==0 and label is 'en'
     nan count(('p', 'u', 't'))==0 and label is 'en'
     nan count(('u', 't', 'e'))==0 and label is 'en'
     nan count(('t', 'e', 'r'))==0 and label is 'en'
-----------------------------------------------hh--------------------------------------
